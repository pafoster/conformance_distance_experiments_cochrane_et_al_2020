{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Anomalous Streams using Signature Variance\n",
    "## PenDigits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import iisignature\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import sklearn.utils\n",
    "\n",
    "sys.path.insert(0, os.path.join('..', 'empirical-outliers'))\n",
    "import variance\n",
    "sys.path.insert(0, os.path.join('..', 'common'))\n",
    "import evaluation\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    " \n",
    "import IPython.display\n",
    "IPython.display.set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract dataset, if it does not already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URLS = ['https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.tes.Z',\n",
    "                'https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.tra.Z']\n",
    "\n",
    "for source_url in DATASET_URLS:\n",
    "    target_filename = source_url.split('/')[-1]\n",
    "    if not os.path.exists(target_filename[:-2]):\n",
    "        try:\n",
    "            util.download(source_url, target_filename)\n",
    "            !uncompress {target_filename}\n",
    "        except:\n",
    "            if os.path.exists(target_filename):\n",
    "                os.remove(target_filename)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and create data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': util.load_pendigits_dataset('pendigits-orig.tra'),\n",
    "        'test': util.load_pendigits_dataset('pendigits-orig.tes')}\n",
    "\n",
    "dataframes = []\n",
    "for subset, data in data.items():\n",
    "    df = pd.DataFrame(data).T\n",
    "    df.columns = ['Stream', 'Digit']\n",
    "    df['Subset'] = subset\n",
    "    dataframes.append(df)\n",
    "df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain summary statistics for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_corpus_size = df[df['Subset'] == 'train']['Digit'].value_counts().mean()\n",
    "testing_data_size = len(df[df['Subset'] == 'test'])\n",
    "mean_outlier_size = testing_data_size - df[df['Subset'] == 'test']['Digit'].value_counts().mean()\n",
    "\n",
    "print('Mean corpus size: {}'.format(mean_corpus_size))\n",
    "print('Testing subset size: {}'.format(testing_data_size))\n",
    "print('Mean testing outlier subset size: {}'.format(mean_outlier_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which takes an input stream and transforms it as specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(raw_stream, include_time=False, lead_lag_transform=False, invisibility_transform=False):\n",
    "    stream = raw_stream\n",
    "\n",
    "    if include_time:\n",
    "        stream = np.column_stack((stream, np.arange(stream.shape[0])))\n",
    "\n",
    "    if lead_lag_transform:\n",
    "        stream = np.repeat(stream, 2, axis=0)\n",
    "        stream = np.column_stack((stream[1:, :], stream[:-1, :]))\n",
    "\n",
    "    if invisibility_transform:\n",
    "        stream = np.vstack(((stream, stream[-1], np.zeros_like(stream[-1]))))\n",
    "        stream = np.column_stack((stream, np.append(np.ones(stream.shape[0]-2), [0, 0])))\n",
    "\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which computes the variance of testing subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stream_signature_variance(corpus, inlier_test, outlier_test, signature_order, random_state=0):    \n",
    "    def normalise(streams):\n",
    "        return [sklearn.preprocessing.MinMaxScaler().fit_transform(stream) for stream in streams]\n",
    "\n",
    "    corpus, inlier_test, outlier_test = map(normalise, (corpus, inlier_test, outlier_test))\n",
    "\n",
    "    variance_inliers = np.array(variance.variance(inlier_test, corpus, signature_order))\n",
    "    variance_outliers = np.array(variance.variance(sklearn.utils.shuffle(outlier_test, random_state=random_state),\n",
    "                                                   corpus, signature_order))\n",
    "    \n",
    "    return variance_inliers, variance_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which computes the variance of testing subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(df, outlier_detection_method=compute_stream_signature_variance, signature_order=3):\n",
    "    variance_inliers = {}\n",
    "    variance_outliers = {}\n",
    "    \n",
    "    key = 'Lead/Lag={}, Time={}, Inv. Reset={}'.format(False, False, False)\n",
    "    stream_function = lambda raw_stream: get_stream(raw_stream, invisibility_transform=False,\n",
    "                                                    include_time=False,\n",
    "                                                    lead_lag_transform=False)\n",
    "\n",
    "    variance_inliers[key] = []\n",
    "    variance_outliers[key] = []\n",
    "\n",
    "    # Iterate over digits, aggregating the variance\n",
    "    for digit, df_by_digit in df.groupby('Digit'):\n",
    "        corpus = df_by_digit[df_by_digit['Subset'] == 'train']['Stream'].apply(stream_function).to_list()\n",
    "        inlier_test = df_by_digit[df_by_digit['Subset'] == 'test']['Stream'].apply(stream_function).to_list()\n",
    "        outlier_test = df[(df['Digit'] != digit) & (df['Subset'] == 'test')]['Stream'].apply(stream_function).to_list()\n",
    "        \n",
    "        # corpus = df[(df['Digit'] != digit) & (df['Subset'] == 'train')]['Stream'].apply(stream_function).to_list()\n",
    "        # inlier_test = df[(df['Digit'] != digit) & (df['Subset'] == 'test')]['Stream'].apply(stream_function).to_list()        \n",
    "        # outlier_test = df_by_digit[df_by_digit['Subset'] == 'test']['Stream'].apply(stream_function).to_list()\n",
    "        \n",
    "        var_inliers, var_outliers = outlier_detection_method(corpus,\n",
    "                                                             inlier_test,\n",
    "                                                             outlier_test,\n",
    "                                                             signature_order,\n",
    "                                                             random_state=digit)\n",
    "\n",
    "        variance_inliers[key] += list(var_inliers)\n",
    "        variance_outliers[key] += list(var_outliers)\n",
    "\n",
    "    variance_inliers[key] = np.array(variance_inliers[key])\n",
    "    variance_outliers[key] = np.array(variance_outliers[key])\n",
    "            \n",
    "    return variance_inliers, variance_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which evaluates across a range of signature orders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def evaluate_across_hyperparams(outlier_detection_method=compute_stream_signature_variance):\n",
    "    variance_inliers_vs_hyperparams = {}\n",
    "    variance_outliers_vs_hyperparams = {}\n",
    "\n",
    "    for signature_order in range(1, 6):\n",
    "        key = 'Signature order {}'.format(signature_order)\n",
    "        variance_inliers_vs_hyperparams[key], variance_outliers_vs_hyperparams[key] = \\\n",
    "            evaluate(df, signature_order=signature_order, outlier_detection_method=outlier_detection_method)\n",
    "\n",
    "    return variance_inliers_vs_hyperparams, variance_outliers_vs_hyperparams\n",
    "\n",
    "variance_inliers_vs_hyperparams, variance_outliers_vs_hyperparams = evaluate_across_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC Curves across hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in variance_inliers_vs_hyperparams:\n",
    "    print(key)\n",
    "    evaluation.plot_roc_curve(variance_inliers_vs_hyperparams[key], variance_outliers_vs_hyperparams[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabulate AUC scores and bootstapped standard errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_vs_hyperparameters, standard_error_vs_hyperparameters = \\\n",
    "    evaluation.tabulate_performance(variance_inliers_vs_hyperparams, variance_outliers_vs_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_vs_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_error_vs_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot empirical cumulative distribution functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(x):\n",
    "    xs = np.sort(x)\n",
    "    ys = np.arange(1, len(xs)+1)/float(len(xs))\n",
    "    return xs, ys\n",
    "\n",
    "def plot_ecdf(variance_inliers, variance_outliers, lower_percentile=1, upper_percentile=100, fontsize=15):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    xs, ys = ecdf(variance_inliers)\n",
    "    plt.plot(xs, ys, label='Normal', linestyle='--', linewidth=3)\n",
    "    xs, ys = ecdf(variance_outliers)\n",
    "    plt.plot(xs, ys, label='Anomalous', linestyle='-', linewidth=3)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Conformance', fontsize=fontsize)\n",
    "    plt.ylabel('Cumulative probability', fontsize=fontsize)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=fontsize)\n",
    "    \n",
    "    # plt.xlim([min(np.percentile(variance_inliers, lower_percentile),\n",
    "    #               np.percentile(variance_outliers, lower_percentile)),\n",
    "    #           max(np.percentile(variance_inliers, upper_percentile),\n",
    "    #               np.percentile(variance_outliers, upper_percentile))])\n",
    "    plt.xlim([10**-6, 10**11])\n",
    "    plt.yticks(np.linspace(0.0, 1.0, 11))\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    \n",
    "    plt.legend(loc='lower right', fontsize=fontsize)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "key = 'Lead/Lag=False, Time=False, Inv. Reset=False'\n",
    "for order in variance_inliers_vs_hyperparams.keys():\n",
    "    fig = plot_ecdf(variance_inliers_vs_hyperparams[order][key],\n",
    "                    variance_outliers_vs_hyperparams[order][key])\n",
    "    fig.savefig('ecdf_order_{}.pdf'.format(order[-1]), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Execution time (s): {:.1f}'.format(time.time() - t0))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
